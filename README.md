# Projeto de AutomaÃ§Ã£o de RelatÃ³rios

Este projeto implementa um pipeline completo de anÃ¡lise de dados com duas versÃµes: uma usando **Pandas** para datasets menores e outra usando **Apache Spark** para processamento de big data.

## ğŸ“ Estrutura do Projeto

```
projeto_automation_reporting/
â”‚
â”œâ”€â”€ data/                    # Dados brutos (CSV de vendas, etc.)
â”œâ”€â”€ reports/                 # RelatÃ³rios gerados
â”‚
â”œâ”€â”€ pandas_version/          # Scripts usando Pandas
â”‚   â”œâ”€â”€ extract.py          # ExtraÃ§Ã£o de dados
â”‚   â”œâ”€â”€ transform.py        # TransformaÃ§Ã£o e limpeza
â”‚   â”œâ”€â”€ anomaly.py          # DetecÃ§Ã£o de anomalias
â”‚   â”œâ”€â”€ report.py           # GeraÃ§Ã£o de relatÃ³rios
â”‚   â””â”€â”€ automation.py       # AutomaÃ§Ã£o do pipeline
â”‚
â”œâ”€â”€ spark_version/           # Scripts usando PySpark
â”‚   â”œâ”€â”€ extract.py          # ExtraÃ§Ã£o de dados
â”‚   â”œâ”€â”€ transform.py        # TransformaÃ§Ã£o e limpeza
â”‚   â”œâ”€â”€ anomaly.py          # DetecÃ§Ã£o de anomalias
â”‚   â”œâ”€â”€ report.py           # GeraÃ§Ã£o de relatÃ³rios
â”‚   â””â”€â”€ automation.py       # AutomaÃ§Ã£o do pipeline
â”‚
â”œâ”€â”€ requirements.txt         # DependÃªncias do projeto
â””â”€â”€ README.md               # Este arquivo
```

## ğŸš€ InstalaÃ§Ã£o

### 1. Clone o repositÃ³rio
```bash
git clone <url-do-repositorio>
cd projeto_automation_reporting
```

### 2. Crie um ambiente virtual
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate     # Windows
```

### 3. Instale as dependÃªncias
```bash
pip install -r requirements.txt
```

### 4. Instale o Apache Spark (apenas para versÃ£o Spark)
```bash
# Baixe o Spark do site oficial: https://spark.apache.org/downloads.html
# Configure as variÃ¡veis de ambiente SPARK_HOME e PATH
```

## ğŸ“Š Funcionalidades

### VersÃ£o Pandas
- **Ideal para**: Datasets atÃ© 1GB, anÃ¡lises rÃ¡pidas
- **CaracterÃ­sticas**: 
  - Processamento em memÃ³ria
  - Interface simples e intuitiva
  - Ideal para prototipagem

### VersÃ£o Spark
- **Ideal para**: Big data, datasets > 1GB
- **CaracterÃ­sticas**:
  - Processamento distribuÃ­do
  - Escalabilidade horizontal
  - OtimizaÃ§Ãµes automÃ¡ticas

## ğŸ”§ MÃ³dulos

### 1. Extract (ExtraÃ§Ã£o)
- ExtraÃ§Ã£o de dados de CSV, Parquet, bancos de dados
- ValidaÃ§Ã£o de dados
- CriaÃ§Ã£o de dados de exemplo

### 2. Transform (TransformaÃ§Ã£o)
- Limpeza de dados (duplicatas, valores ausentes)
- CriaÃ§Ã£o de colunas calculadas
- AgregaÃ§Ãµes e pivots
- DetecÃ§Ã£o e remoÃ§Ã£o de outliers

### 3. Anomaly (DetecÃ§Ã£o de Anomalias)
- **MÃ©todos estatÃ­sticos**: IQR, Z-Score
- **Machine Learning**: Isolation Forest, K-Means
- **Temporal**: DetecÃ§Ã£o de padrÃµes anÃ´malos no tempo
- **RelatÃ³rios**: AnÃ¡lise detalhada de anomalias

### 4. Report (RelatÃ³rios)
- **VisualizaÃ§Ãµes**: GrÃ¡ficos de linha, barras, heatmaps
- **Formatos**: PDF, Excel, imagens PNG
- **MÃ©tricas**: KPIs, estatÃ­sticas descritivas
- **Dashboards**: Resumos executivos

### 5. Automation (AutomaÃ§Ã£o)
- **ExecuÃ§Ã£o Ãºnica**: Pipeline completo em uma execuÃ§Ã£o
- **Agendamento**: ExecuÃ§Ã£o diÃ¡ria automÃ¡tica
- **ConfiguraÃ§Ã£o**: Arquivos JSON para personalizaÃ§Ã£o
- **Logs**: Rastreamento completo de execuÃ§Ãµes

## ğŸ¯ Como Usar

### ExecuÃ§Ã£o RÃ¡pida - VersÃ£o Pandas

```bash
# Executa o pipeline completo uma vez
cd pandas_version
python automation.py --mode once

# Executa em modo agendado (diÃ¡rio Ã s 9h)
python automation.py --mode schedule
```

### ExecuÃ§Ã£o RÃ¡pida - VersÃ£o Spark

```bash
# Executa o pipeline completo uma vez
cd spark_version
python automation.py --mode once

# Executa em modo agendado (diÃ¡rio Ã s 9h)
python automation.py --mode schedule
```

### ExecuÃ§Ã£o Individual dos MÃ³dulos

#### Pandas
```bash
cd pandas_version

# 1. ExtraÃ§Ã£o
python extract.py

# 2. TransformaÃ§Ã£o
python transform.py

# 3. DetecÃ§Ã£o de anomalias
python anomaly.py

# 4. GeraÃ§Ã£o de relatÃ³rios
python report.py
```

#### Spark
```bash
cd spark_version

# 1. ExtraÃ§Ã£o
python extract.py

# 2. TransformaÃ§Ã£o
python transform.py

# 3. DetecÃ§Ã£o de anomalias
python anomaly.py

# 4. GeraÃ§Ã£o de relatÃ³rios
python report.py
```

## âš™ï¸ ConfiguraÃ§Ã£o

### Arquivo de ConfiguraÃ§Ã£o (JSON)
```json
{
    "data_path": "data/",
    "reports_path": "reports/",
    "input_file": "sales_data.csv",
    "output_file": "processed_sales_data.csv",
    "anomaly_threshold": 0.1,
    "enable_anomaly_detection": true,
    "enable_reporting": true,
    "schedule_time": "09:00",
    "retention_days": 30
}
```

### Usando ConfiguraÃ§Ã£o Personalizada
```bash
python automation.py --mode once --config config.json
```

## ğŸ“ˆ Exemplo de Dados

O projeto inclui geraÃ§Ã£o automÃ¡tica de dados de exemplo com:
- **Vendas**: ID, data, produto, quantidade, preÃ§o, total
- **PerÃ­odo**: 1000 transaÃ§Ãµes distribuÃ­das ao longo do tempo
- **Produtos**: 5 produtos diferentes
- **PadrÃµes**: VariaÃ§Ãµes sazonais e anomalias

## ğŸ“Š RelatÃ³rios Gerados

### PDF
- Resumo executivo
- GrÃ¡ficos de vendas temporais
- Top produtos
- DistribuiÃ§Ã£o de vendas
- AnÃ¡lise de categorias

### Excel
- Dados originais
- Resumo de mÃ©tricas
- Top produtos
- Vendas mensais
- EstatÃ­sticas por categoria

### Imagens
- GrÃ¡fico de vendas ao longo do tempo
- Top 10 produtos
- DistribuiÃ§Ã£o de valores
- Heatmap produto x mÃªs
- GrÃ¡fico de pizza por categoria

## ğŸ” DetecÃ§Ã£o de Anomalias

### MÃ©todos Implementados

1. **IQR (Interquartile Range)**
   - Detecta outliers baseado em quartis
   - Threshold configurÃ¡vel (padrÃ£o: 1.5)

2. **Z-Score**
   - Detecta valores que se desviam significativamente da mÃ©dia
   - Threshold configurÃ¡vel (padrÃ£o: 3)

3. **Isolation Forest**
   - Algoritmo de ML para detecÃ§Ã£o de anomalias
   - ContaminaÃ§Ã£o configurÃ¡vel (padrÃ£o: 10%)

4. **K-Means Clustering**
   - Agrupa dados e identifica pontos distantes dos centrÃ³ides
   - NÃºmero de clusters configurÃ¡vel

5. **AnÃ¡lise Temporal**
   - Detecta padrÃµes anÃ´malos ao longo do tempo
   - Baseado em mÃ©dia mÃ³vel e desvio padrÃ£o

## ğŸš¨ Logs e Monitoramento

### Arquivos de Log
- `automation.log` - Logs da versÃ£o Pandas
- `spark_automation.log` - Logs da versÃ£o Spark

### InformaÃ§Ãµes Registradas
- InÃ­cio e fim de cada etapa
- Contagem de registros processados
- Erros e exceÃ§Ãµes
- Tempo de execuÃ§Ã£o
- ConfiguraÃ§Ãµes utilizadas

## ğŸ› ï¸ Desenvolvimento

### Estrutura de Classes

#### Pandas
- `DataExtractor`: ExtraÃ§Ã£o de dados
- `DataTransformer`: TransformaÃ§Ã£o de dados
- `AnomalyDetector`: DetecÃ§Ã£o de anomalias
- `ReportGenerator`: GeraÃ§Ã£o de relatÃ³rios
- `DataPipeline`: OrquestraÃ§Ã£o do pipeline

#### Spark
- `SparkDataExtractor`: ExtraÃ§Ã£o com Spark
- `SparkDataTransformer`: TransformaÃ§Ã£o com Spark
- `SparkAnomalyDetector`: DetecÃ§Ã£o com Spark
- `SparkReportGenerator`: RelatÃ³rios com Spark
- `SparkDataPipeline`: Pipeline Spark

### Extensibilidade

O projeto foi projetado para ser facilmente extensÃ­vel:

1. **Novos formatos de dados**: Adicione mÃ©todos em `extract.py`
2. **Novas transformaÃ§Ãµes**: Estenda `transform.py`
3. **Novos algoritmos de anomalia**: Implemente em `anomaly.py`
4. **Novos tipos de relatÃ³rio**: Adicione em `report.py`

## ğŸ“‹ Requisitos do Sistema

### MÃ­nimos
- Python 3.8+
- 4GB RAM
- 1GB espaÃ§o em disco

### Recomendados
- Python 3.10+
- 8GB RAM
- 5GB espaÃ§o em disco
- Apache Spark 3.4+ (para versÃ£o Spark)

## ğŸ¤ ContribuiÃ§Ã£o

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ğŸ“ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo `LICENSE` para mais detalhes.

## ğŸ†˜ Suporte

Para dÃºvidas ou problemas:

1. Verifique os logs de execuÃ§Ã£o
2. Consulte a documentaÃ§Ã£o dos mÃ³dulos
3. Abra uma issue no repositÃ³rio
4. Entre em contato com a equipe de desenvolvimento

## ğŸ”„ AtualizaÃ§Ãµes Futuras

- [ ] Interface web para configuraÃ§Ã£o
- [ ] Suporte a mais formatos de dados
- [ ] IntegraÃ§Ã£o com bancos de dados NoSQL
- [ ] Alertas por email/Slack
- [ ] Dashboard em tempo real
- [ ] Machine Learning avanÃ§ado
- [ ] Suporte a streaming de dados

---

**Desenvolvido com â¤ï¸ para automaÃ§Ã£o de relatÃ³rios de dados**
